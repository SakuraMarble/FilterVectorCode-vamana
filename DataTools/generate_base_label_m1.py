import numpy as np
from collections import defaultdict, Counter
from itertools import combinations

def generate_unique_attr_vectors(n, m, k, required_attrs, min_coverage):
    """
    å±æ€§ç”Ÿæˆæ–¹æ³•ï¼Œç¡®ä¿æ‰€æœ‰å¿…éœ€å±æ€§éƒ½è¾¾åˆ°æœ€å°è¦†ç›–ç‡ï¼ˆå±æ€§å€¼èŒƒå›´ï¼š1~kï¼‰
    å‚æ•°ï¼š
        n: æ€»å‘é‡æ•°
        m: æ¯ä¸ªå‘é‡çš„å±æ€§æ•°
        k: å±æ€§æœ€å¤§å€¼ï¼ˆå±æ€§å€¼èŒƒå›´ä¸º 1~kï¼‰
        required_attrs: éœ€è¦ä¿è¯è¦†ç›–ç‡çš„å±æ€§åˆ—è¡¨
        min_coverage: æ¯ä¸ªå±æ€§éœ€è¦çš„æœ€å°è¦†ç›–ç‡(0-1)
    """
    # å‚æ•°æ ¡éªŒ
    if len(required_attrs) > m:
        raise ValueError(f"æ— æ³•æ»¡è¶³è¦æ±‚ï¼šæ¯ä¸ªå‘é‡åªæœ‰{m}ä¸ªå±æ€§ï¼Œä½†éœ€è¦ä¿è¯{len(required_attrs)}ä¸ªå±æ€§çš„è¦†ç›–ç‡")
    
    if any(attr < 1 or attr > k for attr in required_attrs):
        raise ValueError("required_attrs ä¸­çš„å±æ€§å¿…é¡»åœ¨ 1 åˆ° k èŒƒå›´å†…")

    data = []
    attr_counts = {a: 0 for a in required_attrs}
    target_count = int(n * min_coverage)
    
    # ç¬¬ä¸€é˜¶æ®µï¼šä¸ºæ¯ä¸ªå¿…éœ€å±æ€§åˆ›å»ºä¸“ç”¨å‘é‡
    for a in required_attrs:
        needed = max(0, target_count - attr_counts[a])
        
        # åˆ›å»ºåŒ…å«è¯¥å±æ€§çš„å‘é‡ï¼Œå°½å¯èƒ½åŒ…å«å…¶ä»–å¿…éœ€å±æ€§
        for _ in range(needed):
            other_req_attrs = [ra for ra in required_attrs 
                             if ra != a and attr_counts[ra] < target_count]
            num_other_req = min(len(other_req_attrs), m-1)
            
            # é€‰æ‹©å…¶ä»–å±æ€§
            other_attrs = []
            if num_other_req > 0:
                other_attrs = list(np.random.choice(other_req_attrs, size=num_other_req, replace=False))
            
            # è¡¥å……éšæœºå±æ€§
            remaining = m - 1 - len(other_attrs)
            if remaining > 0:
                available = [x for x in range(1, k+1) if x not in [a] + other_attrs]
                other_attrs.extend(np.random.choice(available, size=remaining, replace=False))
            
            vec = np.array([a] + other_attrs)
            np.random.shuffle(vec)
            data.append(vec)
            
            # æ›´æ–°è®¡æ•°
            for attr in vec:
                if attr in attr_counts:
                    attr_counts[attr] += 1
    
    # ç¬¬äºŒé˜¶æ®µï¼šå¡«å……å‰©ä½™å‘é‡
    while len(data) < n:
        under_covered = [a for a in required_attrs if attr_counts[a] < target_count]
        
        if under_covered:
            a = np.random.choice(under_covered)
            available = [x for x in range(1, k+1) if x != a]
            other_attrs = np.random.choice(available, size=m-1, replace=False)
            vec = np.append(a, other_attrs)
        else:
            vec = np.random.choice(range(1, k+1), size=m, replace=False)
        
        np.random.shuffle(vec)
        data.append(vec)
        
        # æ›´æ–°è®¡æ•°
        for a in required_attrs:
            if a in vec:
                attr_counts[a] += 1
    
    # æœ€ç»ˆå¼ºåˆ¶ä¿®æ­£
    for a in required_attrs:
        if attr_counts[a] < target_count:
            deficit = target_count - attr_counts[a]
            candidates = [i for i, vec in enumerate(data) if a not in vec]
            
            for i in np.random.choice(candidates, size=deficit, replace=False):
                replaceable = [x for x in data[i] if x not in required_attrs or 
                             (x in required_attrs and attr_counts[x] > target_count)]
                if not replaceable:
                    continue
                
                to_replace = np.random.choice(replaceable)
                data[i] = np.where(data[i] == to_replace, a, data[i])
                attr_counts[a] += 1
                if to_replace in attr_counts:
                    attr_counts[to_replace] -= 1
    
    # éªŒè¯
    attr_counts = {a: 0 for a in required_attrs}
    for vec in data:
        for a in required_attrs:
            if a in vec:
                attr_counts[a] += 1
    
    print("\næœ€ç»ˆå±æ€§è¦†ç›–ç‡éªŒè¯:")
    for a in required_attrs:
        coverage = attr_counts[a] / n
        print(f"å±æ€§ {a}: {coverage:.2%} (ç›®æ ‡: {min_coverage:.0%})")
        if coverage < min_coverage:
            raise RuntimeError(f"æ— æ³•æ»¡è¶³å±æ€§ {a} çš„è¦†ç›–ç‡è¦æ±‚")
    
    return data

def map_to_groups_with_ids(data):
    """
    å°†åŸå§‹å‘é‡æŒ‰å±æ€§é›†åˆåˆ†ç»„ï¼Œå¹¶è®°å½•æ¯ä¸ª group æ¥è‡ªå“ªäº›åŸå§‹å‘é‡çš„ç´¢å¼•ï¼ˆIDï¼‰
    è¿”å›ï¼š
        groups: list of frozensetï¼Œæ¯ä¸ªæ˜¯ä¸€ä¸ªå”¯ä¸€çš„ group
        group_id_map: dictï¼Œgroup -> group ID
        group_members: list of listï¼Œæ¯ä¸ªå…ƒç´ æ˜¯å±äºè¯¥ group çš„åŸå§‹å‘é‡ ID åˆ—è¡¨
    """
    from collections import defaultdict

    group_members = defaultdict(list)  # æ¯ä¸ª group å¯¹åº”çš„åŸå§‹å‘é‡ ID åˆ—è¡¨

    for idx, vec in enumerate(data):
        group = frozenset(vec)
        group_members[group].append(idx)

    groups = list(group_members.keys())
    group_id_map = {group: gid for gid, group in enumerate(groups)}
    members_list = [group_members[group] for group in groups]

    return groups, group_id_map, members_list

def count_group_with_attributes(groups, attrs=[0, 1, 2]):
    """
    ç»Ÿè®¡æœ‰å¤šå°‘ group åŒ…å«æŒ‡å®šå±æ€§
    """
    counts = {a: 0 for a in attrs}
    total = len(groups)

    for group in groups:
        for a in attrs:
            if a in group:
                counts[a] += 1

    print("Group ä¸­åŒ…å«æŒ‡å®šå±æ€§çš„æ¯”ä¾‹ï¼š")
    for a in attrs:
        print(f"å±æ€§ {a}å‡ºç°æ¬¡æ•°: {counts[a]}")
        print(f"æ€»ç»„æ•°: {total}")
        print(f"å±æ€§ {a}: {counts[a]/total:.2%}")
    return counts

def build_inverted_index_for_groups(groups):
    """
    æ„å»ºå€’æ’ç´¢å¼•ï¼šå±æ€§ -> group ID åˆ—è¡¨
    """
    index = defaultdict(list)
    for idx, group in enumerate(groups):
        for val in group:
            index[val].append(idx)
    return index

def analyze_query_coverage(index, total_groups, attrs=[0, 1, 2]):
    """
    åˆ†ææŸ¥è¯¢è¦†ç›–ç‡ï¼šæŸ¥è¯¢æŸä¸ªå±æ€§èƒ½å‘½ä¸­å¤šå°‘ä¸ª group
    """
    print("\næŸ¥è¯¢è¦†ç›–ç‡åˆ†æï¼š")
    for a in attrs:
        coverage = len(set(index[a])) / total_groups
        print(f"æŸ¥è¯¢å±æ€§ {a} èƒ½å®šä½åˆ° {coverage:.2%} çš„ entry point")

def save_vectors_to_txt(data, filename="vectors.txt"):
    """
    å°†ç”Ÿæˆçš„å‘é‡æ•°æ®ä¿å­˜ä¸º txt æ–‡ä»¶
    æ¯è¡Œæ˜¯ä¸€ä¸ªå‘é‡ï¼Œå±æ€§ä¹‹é—´ç”¨ç©ºæ ¼åˆ†éš”
    """
    with open(filename, 'w') as f:
        for vec in data:
            line = ','.join(map(str, sorted(vec)))  # æ’åºä¾¿äºæŸ¥çœ‹
            f.write(line + '\n')
    print(f"Vectors saved to {filename}")

def print_group_details(groups, members_list):
    print("\nğŸ“Š Group Details:")
    for gid, (group, members) in enumerate(zip(groups, members_list)):
        print(f"Group {gid}:")
        print(f"  å±æ€§é›†åˆ: {sorted(group)}")
        print(f"  å‘é‡æ•°é‡: {len(members)}")
        print(f"  åŸå§‹å‘é‡ç´¢å¼•: {members}")

def print_attribute_to_groups_map(groups, inverted_index):
    print("\nğŸ·ï¸ Attribute to Groups Mapping:")
    for attr, group_ids in sorted(inverted_index.items()):
        print(f"å±æ€§ {attr}:,å‡ºç°åœ¨ {len(group_ids)} ä¸ª group")
        print(f"  å‡ºç°åœ¨ä»¥ä¸‹ group(s): {group_ids}")
        print(f"  group å†…å®¹: {[sorted(groups[gid]) for gid in group_ids]}")
# -----------------------------
# ç¤ºä¾‹è¿è¡Œå‡½æ•°
# -----------------------------

def run_example(dataset_name="arxiv", n=20, m=4, k=8, required_attrs=[0, 1, 2], min_coverage=0.3):
    print(f"\n--- Running Example for Dataset: {dataset_name} ---")

    # è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯å¤ç°æ€§
    np.random.seed(42)

    # Step 1: ç”Ÿæˆæ•°æ® (ç¡®ä¿æ»¡è¶³è¦†ç›–ç‡è¦æ±‚)
    print("Step 1: Generating data with coverage requirements...")
    data = generate_unique_attr_vectors(n, m, k, required_attrs, min_coverage)

    # æ–°å¢ï¼šä¿å­˜æ•°æ®åˆ° txt æ–‡ä»¶
    save_vectors_to_txt(data, filename=f"{dataset_name}_base_labels.txt")
    
    # Step 2: åˆ†ç»„
    print("Step 2: Mapping to groups (f-points)...")
    groups, group_id_map, members_list = map_to_groups_with_ids(data)
    total_groups = len(groups)
    print(f"Total number of f-points (groups): {total_groups}")

   #  # æ–°å¢ï¼šæ‰“å° group è¯¦æƒ…
   #  print_group_details(groups, members_list)

    # Step 3: å±æ€§ç»Ÿè®¡
    print("Step 3: Counting attributes in groups...")
    counts = count_group_with_attributes(groups, required_attrs)
    
    # éªŒè¯groupçº§åˆ«çš„è¦†ç›–ç‡
    for a in required_attrs:
        coverage = counts[a] / total_groups
        assert coverage >= min_coverage, f"å±æ€§ {a} åœ¨groupçº§åˆ«çš„è¦†ç›–ç‡ä¸è¶³"

    # Step 4: å»ºç«‹å€’æ’ç´¢å¼•
    print("Step 4: Building inverted index for querying...")
    inverted_index = build_inverted_index_for_groups(groups)

    # Step 5: æŸ¥è¯¢è¦†ç›–ç‡åˆ†æ
    print("Step 5: Analyzing query coverage...")
    analyze_query_coverage(inverted_index, total_groups, required_attrs)

    print(f"\n--- End of Example for Dataset: {dataset_name} ---")
    return {
        'data': data,
        'groups': groups,
        'inverted_index': inverted_index,
        'total_groups': total_groups
    }
# -----------------------------
# ä¸»ç¨‹åºå…¥å£
# -----------------------------

if __name__ == "__main__":
    # ç”Ÿæˆnä¸ªå‘é‡ï¼Œæ¯ä¸ªmä¸ªå±æ€§ï¼Œå±æ€§å€¼èŒƒå›´0-k,è¦æ±‚å±æ€§required_attrsåœ¨æ¯ä¸ªgroupä¸­çš„è¦†ç›–ç‡è‡³å°‘min_coverage
    results_arxiv = run_example("arxiv", n=157606, m=9, k=100, required_attrs=[1,2,3,4], min_coverage=0.7)